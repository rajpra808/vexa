name: vexa_dev
services:
  api-gateway:
    build:
      context: .
      dockerfile: services/api-gateway/Dockerfile
    ports:
      - "${API_GATEWAY_HOST_PORT:-8056}:8000"
    environment:
      - ADMIN_API_URL=http://admin-api:8001
      - BOT_MANAGER_URL=http://bot-manager:8080
      - TRANSCRIPTION_COLLECTOR_URL=http://transcription-collector:8000
      - MCP_URL=http://mcp:18888
      - REDIS_URL=redis://redis:6379/0
      # Used to generate public share URLs for ChatGPT "Read from <url>" flows
      - PUBLIC_BASE_URL=https://share.dev.vexa.ai
      - TRANSCRIPT_SHARE_TTL_SECONDS=900
      - LOG_LEVEL=DEBUG
    volumes:
      - ../.cursor:/app/.cursor
    init: true
    depends_on:
      admin-api:
        condition: service_started
      bot-manager:
        condition: service_started
      transcription-collector:
        condition: service_started
    networks:
      - vexa_default
    restart: unless-stopped

  admin-api:
    build:
      context: .
      dockerfile: services/admin-api/Dockerfile
    ports:
      - "${ADMIN_API_HOST_PORT:-8057}:8001"
    environment:
      - DB_HOST=${DB_HOST:-postgres}
      - DB_PORT=${DB_PORT:-5432}
      - DB_NAME=${DB_NAME:-vexa}
      - DB_USER=${DB_USER:-postgres}
      - DB_PASSWORD=${DB_PASSWORD:-postgres}
      - DB_SSL_MODE=${DB_SSL_MODE:-disable}
      - ADMIN_API_TOKEN=${ADMIN_API_TOKEN}
      - LOG_LEVEL=DEBUG
    init: true
    networks:
      - vexa_default
    restart: unless-stopped

  bot-manager:
    build:
      context: .
      dockerfile: services/bot-manager/Dockerfile
    environment:
      - REDIS_URL=redis://redis:6379/0
      - BOT_IMAGE_NAME=${BOT_IMAGE_NAME:-vexa-bot:dev}
      - DOCKER_NETWORK=${COMPOSE_PROJECT_NAME:-vexa}_vexa_default
      - LOG_LEVEL=DEBUG
      - DB_HOST=${DB_HOST:-postgres}
      - DB_PORT=${DB_PORT:-5432}
      - DB_NAME=${DB_NAME:-vexa}
      - DB_USER=${DB_USER:-postgres}
      - DB_PASSWORD=${DB_PASSWORD:-postgres}
      - DB_SSL_MODE=${DB_SSL_MODE:-disable}
      - DOCKER_HOST=unix://var/run/docker.sock
      - DEVICE_TYPE=${DEVICE_TYPE}
      - WHISPER_LIVE_URL=${WHISPER_LIVE_URL:-ws://whisperlive-remote:9090/ws}
      - ADMIN_TOKEN=${ADMIN_API_TOKEN}
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ../.cursor:/app/.cursor
    init: true
    depends_on:
      redis:
        condition: service_started
    networks:
      - vexa_default
    restart: unless-stopped

  whisperlive:
    profiles: ["gpu"]
    build:
      context: .
      dockerfile: services/WhisperLive/Dockerfile.project
    volumes:
      - ./hub:/root/.cache/huggingface/hub
      - ./services/WhisperLive/models:/app/models
    environment:
      # Use Redis Stream URL instead of WebSocket URL
      - REDIS_STREAM_URL=redis://redis:6379/0/transcription_segments
      # Keep the old URL for backward compatibility
      - TRANSCRIPTION_COLLECTOR_URL=redis://redis:6379/0/transcription_segments
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=0
      - REDIS_STREAM_NAME=transcription_segments
      - LANGUAGE_DETECTION_SEGMENTS=${LANGUAGE_DETECTION_SEGMENTS}
      - DEVICE_TYPE=${DEVICE_TYPE}
      - WHISPER_MODEL_SIZE=${WHISPER_MODEL_SIZE}
      - WL_MAX_CLIENTS=${WL_MAX_CLIENTS}
      # Server-level speaker-based circuit breaker
      - WL_USE_SPEAKER_GROUND_TRUTH=${WL_USE_SPEAKER_GROUND_TRUTH:-true}
      - WL_SERVER_SPEAKER_NO_TX_STALL_S=${WL_SERVER_SPEAKER_NO_TX_STALL_S:-30}
      - WL_SPEAKER_ACTIVE_WINDOW_S=${WL_SPEAKER_ACTIVE_WINDOW_S:-8}
      - WL_SERVER_WARMUP_S=${WL_SERVER_WARMUP_S:-60}
      # VAD filter - most permissive to catch hallucinations during silence
      - VAD_FILTER_THRESHOLD=${VAD_FILTER_THRESHOLD:-0}
      # Minimum audio duration (seconds) before triggering transcription - lower = lower latency
      - MIN_AUDIO_S=${MIN_AUDIO_S:-2.0}
      - CONSUL_ENABLE=false
      - WL_REDIS_DISCOVERY_ENABLED=false
    command: --port 9090 --backend faster_whisper --faster_whisper_custom_model_path ${WHISPER_MODEL_SIZE} --min_audio_s ${MIN_AUDIO_S:-2.0}
    expose:
      - "9090" #use for transcription web socket
      - "9091" #use for health check
    deploy:
      replicas: 1
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["3"] # Ensure this is configurable or correct for your setup
              capabilities: [gpu]
    init: true
    depends_on:
      transcription-collector:
        condition: service_started
    networks:
      - vexa_default
  # CPU version of WhisperLive for users without GPU
  whisperlive-cpu:
    profiles: ["cpu"]
    build:
      context: .
      dockerfile: services/WhisperLive/Dockerfile.cpu
    volumes:
      - ./hub:/root/.cache/huggingface/hub
      - ./services/WhisperLive/models:/app/models
    environment:
      - REDIS_STREAM_URL=redis://redis:6379/0/transcription_segments
      - TRANSCRIPTION_COLLECTOR_URL=redis://redis:6379/0/transcription_segments
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=0
      - REDIS_STREAM_NAME=transcription_segments
      - LANGUAGE_DETECTION_SEGMENTS=${LANGUAGE_DETECTION_SEGMENTS}
      - VAD_FILTER_THRESHOLD=${VAD_FILTER_THRESHOLD}
      # Minimum audio duration (seconds) before triggering transcription - lower = lower latency
      - MIN_AUDIO_S=${MIN_AUDIO_S:-2.0}
      - DEVICE_TYPE=cpu
      - WHISPER_MODEL_SIZE=${WHISPER_MODEL_SIZE}
      - WL_MAX_CLIENTS=${WL_MAX_CLIENTS}
      - CONSUL_ENABLE=false
    deploy:
      replicas: 1
    command: --port 9090 --backend faster_whisper --faster_whisper_custom_model_path ${WHISPER_MODEL_SIZE} --min_audio_s ${MIN_AUDIO_S:-2.0}
    expose:
      - "9090" #use for transcription web socket
      - "9091" #use for health check
    init: true
    depends_on:
      transcription-collector:
        condition: service_started
    networks:
      - vexa_default
    # Don't auto-start CPU version, users can manually start it
    restart: unless-stopped

  # Remote API version of WhisperLive
  whisperlive-remote:
    profiles: ["remote"]
    build:
      context: .
      dockerfile: services/WhisperLive/Dockerfile.cpu
    volumes:
      - ./hub:/root/.cache/huggingface/hub
      - ./services/WhisperLive/models:/app/models
    environment:
      - REDIS_STREAM_URL=redis://redis:6379/0/transcription_segments
      - TRANSCRIPTION_COLLECTOR_URL=redis://redis:6379/0/transcription_segments
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=0
      - REDIS_STREAM_NAME=transcription_segments
      - LANGUAGE_DETECTION_SEGMENTS=${LANGUAGE_DETECTION_SEGMENTS}
      - VAD_FILTER_THRESHOLD=${VAD_FILTER_THRESHOLD}
      # Minimum audio duration (seconds) before triggering transcription - lower = lower latency
      - MIN_AUDIO_S=${MIN_AUDIO_S:-2.0}
      # Reconfirmation threshold - lower values reduce latency by reconfirming segments faster
      - SAME_OUTPUT_THRESHOLD=${SAME_OUTPUT_THRESHOLD:-3}
      - DEVICE_TYPE=remote
      - REMOTE_TRANSCRIBER_URL=${REMOTE_TRANSCRIBER_URL}
      - REMOTE_TRANSCRIBER_API_KEY=${REMOTE_TRANSCRIBER_API_KEY}
      - CONSUL_ENABLE=false
    deploy:
      replicas: 1
    command: --port 9090 --backend remote
    expose:
      - "9090" #use for transcription web socket
      - "9091" #use for health check
    init: true
    depends_on:
      transcription-collector:
        condition: service_started
    networks:
      - vexa_default
      - vexa-network
    restart: unless-stopped

  # Deepgram streaming transcription proxy
  deepgram-transcriber:
    profiles: ["deepgram"]
    build:
      context: .
      dockerfile: services/deepgram-transcriber/Dockerfile
    environment:
      - DEEPGRAM_API_KEY=${DEEPGRAM_API_KEY}
      - REDIS_STREAM_URL=redis://redis:6379/0/transcription_segments
      - REDIS_URL=redis://redis:6379/0
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=0
      - REDIS_STREAM_KEY=transcription_segments
      - REDIS_SPEAKER_EVENTS_RELATIVE_STREAM_KEY=speaker_events_relative
    expose:
      - "9090"
    init: true
    depends_on:
      redis:
        condition: service_started
    networks:
      - vexa_default
    restart: unless-stopped

  transcription-collector:
    build:
      context: .
      dockerfile: services/transcription-collector/Dockerfile
    ports:
      - "${TRANSCRIPTION_COLLECTOR_HOST_PORT:-8123}:8000"
    volumes:
      - ./alembic.ini:/app/alembic.ini
      - ./libs/shared-models/alembic:/app/alembic
    environment:
      - DB_HOST=${DB_HOST:-postgres}
      - DB_PORT=${DB_PORT:-5432}
      - DB_NAME=${DB_NAME:-vexa}
      - DB_USER=${DB_USER:-postgres}
      - DB_PASSWORD=${DB_PASSWORD:-postgres}
      - DB_SSL_MODE=${DB_SSL_MODE:-disable}
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_STREAM_NAME=transcription_segments
      - REDIS_CONSUMER_GROUP=collector_group
      - REDIS_STREAM_READ_COUNT=10
      - REDIS_STREAM_BLOCK_MS=2000
      - ADMIN_TOKEN=${ADMIN_API_TOKEN}
      - BACKGROUND_TASK_INTERVAL=10
      - IMMUTABILITY_THRESHOLD=30
      - REDIS_SEGMENT_TTL=3600
      - REDIS_CLEANUP_THRESHOLD=86400
      - LOG_LEVEL=DEBUG
    init: true
    depends_on:
      redis:
        condition: service_started
    networks:
      - vexa_default
    restart: unless-stopped

  mcp:
    build:
      context: .
      dockerfile: services/mcp/Dockerfile
    ports:
      - "${MCP_HOST_PORT:-18888}:18888"
    environment:
      - API_GATEWAY_URL=http://api-gateway:8000
      - LOG_LEVEL=DEBUG
    init: true
    depends_on:
      api-gateway:
        condition: service_started
    networks:
      - vexa_default
    restart: unless-stopped

  redis:
    image: redis:7.0-alpine
    command:
      ["redis-server", "--appendonly", "yes", "--appendfsync", "everysec"]
    volumes:
      - redis-data:/data
    networks:
      - vexa_default
    restart: unless-stopped

volumes:
  redis-data:
  postgres-data:

networks:
  vexa_default:
    driver: bridge
  vexa-network:
    external: true
    name: vexa-network
